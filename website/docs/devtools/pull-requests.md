---
id: pull-requests
title: Pull Requests Flow 
sidebar_label: Pull requests flow 
---

##  Introduction

One day you submit a pull request to Prysm’s GitHub repository, containing a brand new feature 

- The ability to attach a letter to Santa Claus into a block in case he runs a node

Of course it gets approved, and some time later your feature gets included in the next new version of Prysm. But how did it get there? This article explains the process from an infrastructural point of view, outlining all the different tooling that is used to make sure that a new version of the system is stable and ready to be shipped.

###   Google Cloud Platform

All Prysm infrastructure is hosted on the Google Cloud Platform. VM instances, storage and the Google Kubernetes Engine are the primary resources used to accompany a pull request on its journey from GitHub to an official release.


![Platform](/img/platform1.png)

###   Buildkite CI Pipeline

Once a pull request is submitted to GitHub, several checks are performed by the GitHub CI tool. Apart from a few Go-specific verification steps, the rest is run inside a Buildkite CI pipeline.

![Pull](/img/pull1.png)

![Platform2](/img/pull2.png)

Items in this pipeline can be broadly divided into the following categories:

###   Build & Test

Including building the whole project and running all kinds of tests (e.g. unit, end-to-end, fuzz). All of this is done in a Bazel sandboxed environment.
![Build](/img/build1.png)

###   Verify binaries

Make sure that cross-compilation works as expected. Bazel supports linux_amd64, linux_arm64, osx_amd64 and windows_amd64 compilation targets.
![Verify](/img/verify1.png)

###   Generate Docker images

Generate Docker images for the beacon chain, validator and slasher using Bazel. Appropriate tags are applied to each image to make sure other tools can easily find the image that they need.
![Docker](/img/docker1.png)

###   Google Container Registry (GCR) and DockerHub

Docker images generated by Bazel are distributed to appropriate directories in the GCR. This registry contains all the historical versions of Prysm images. The latest images for each component of Prysm are always tagged with ‘latest’.
![Container](/img/google1.png)

Images are also distributed to DockerHub where they are publicly available.
![Container2](/img/google2.png)

##  Spinnaker, Kubernetes and Helm

Once the image is uploaded into GCR, it is time to put it under real-life test scenarios. Even though the Buildkite CI did a comprehensive analysis of Prysm’s code, all of this was done without actually running the program. We want to make sure that users downloading the new version of Prysm do not experience bugs or unexpected errors. 

To achieve high degree of confidence that everything works as expected, we utilize Spinnaker to deploy Prysm to a Kubernetes cluster, where it is run in real-world conditions, sometimes for several days before we decide that the new version is suitable for releasing. This proves with a high probability that no regression has been introduced in pull requests which were merged between the old and new versions. Some pull requests touch critical paths of the system (e.g. consensus code) and it is very important to make sure that such fragile components still function as intended.
![Spinnaker](/img/spin1.png)

All of this is configured with Spinnaker pipelines. Uploading an image into GCR releases a trigger that kicks off the pipeline.
![Spinnaker2](/img/spin2.png)

The pipeline itself consists of several stages, each of which may have one or more tasks. All pipeline tasks can be either sequential, parallel or both (the last one being the case where a task has to wait for some other task to complete, but it can start running while another task in the same stage is running). 
![Spinnaker3](/img/spin3.png)

The most important types of tasks are:

###   Baking manifests

All Kubernetes resources are created from Kubernetes manifests, which describe the resource. Ordinary manifests are somewhat limited as they are not parameterizable, which leads to a lot of duplication and makes it hard to inject configuration values dynamically when running Spinnaker pipelines. That’s why we use the Helm template language for our manifests. Helm templates are YAML-formatted resource descriptions that Kubernetes can understand, and we can take advantage of their flexibility by creating a single Helm template for multiple deployment environments.
![Baking](/img/baking1.png)


###   Canary testing

Canary testing is a very important part of our pipeline. We want to make sure that the new deployment is operating at least as well as the old one, using key metrics that are chosen when the canary is configured. If the passing threshold is met, the pipeline can continue. Otherwise it is either considered failed and cleaned up, or it is up to the developer whether it should continue or not. The former happens when the marginal threshold is not met, the latter when the canary result falls between the two thresholds.
![Canary](/img/canary1.png)


###   Deployment

Some parts of the pipeline require deployment to Kubernetes, including deploying Prysm once the canary is successful. This can be easily done with Spinnaker by applying appropriate configuration to the deployment task.
![Deployment](/img/deploy1.png)


Apart from the types of tasks mentioned above, the pipeline also includes some preparation, cleanup and notification steps, all of which make sure that we get the best possible experience out of rolling out a new version of Prysm.

